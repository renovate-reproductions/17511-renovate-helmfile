---
environments:
  dev-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl
  stage-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl
  core-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl
  it-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl
  poc-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl
  prod-01:
    values:
    - environments/{{ .Environment.Name }}-values.yaml.gotmpl

---
helmDefaults:
  kubeContext: {{ .Environment.Name }}

repositories:
- name: jetstack
  url: https://charts.jetstack.io
- name: incubator
  url: https://charts.helm.sh/incubator
- name: bitnami
  url: https://charts.bitnami.com/bitnami
- name: traefik
  url: https://helm.traefik.io/traefik
- name: aws-ebs-csi-driver
  url: https://kubernetes-sigs.github.io/aws-ebs-csi-driver
- name: aws-efs-csi-driver
  url: https://kubernetes-sigs.github.io/aws-efs-csi-driver
- name: eks
  url: https://aws.github.io/eks-charts
- name: prometheus-community
  url: https://prometheus-community.github.io/helm-charts
- name: fairwinds-stable
  url: https://charts.fairwinds.com/stable
- name: piraeus-charts
  url: https://piraeus.io/helm-charts
- name: kyverno
  url: https://kyverno.github.io/kyverno
- name: kyverno-policy-reporter
  url: https://kyverno.github.io/policy-reporter
- name: autoscaler
  url: https://kubernetes.github.io/autoscaler
- name: deliveryhero
  url: https://charts.deliveryhero.io/
- name: grafana
  url: https://grafana.github.io/helm-charts
- name: indeedeng-alpha
  url: git+https://github.com/indeedeng-alpha/harbor-container-webhook@deploy/charts/harbor-container-webhook?ref=main

releases:
- name: prometheus
  namespace: monitoring
  chart: prometheus-community/kube-prometheus-stack
  version: 39.7.0
  disableValidationOnInstall: true
  needs:
  - ingress/traefik-private
  values:
  - grafana:
      enabled: false
    alertmanager:
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
          traefik.ingress.kubernetes.io/router.middlewares: ingress-platform-standard@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt
        ingressClassName: traefik-private
        pathType: Prefix
        hosts:
        - alertmanager.{{ .Environment.Name }}.example.net
        tls:
        - hosts:
          - alertmanager.{{ .Environment.Name }}.example.net
          secretName: alertmanager-tls
      podDisruptionBudget:
        enabled: true
      alertmanagerSpec:
        replicas: 2
        logFormat: json
        resources:
          requests:
            cpu: 10m
            memory: 36Mi
          limits:
            cpu: 200m
            memory: 256Mi
    prometheus:
      prometheusSpec:
        retention: 30d
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        enableFeatures:
        - memory-snapshot-on-shutdown
        externalLabels:
          cluster: {{ .Environment.Name }}
        logFormat: json
        {{- if (.Values | get "prom.remote_write_url" "") }}
        remoteWrite:
          - url: "{{ .Values.prom.remote_write_url }}"
            queueConfig:
              capacity: 10000
              minShards: 10
              maxShards: 100
              maxSamplesPerSend: 1000
        {{- end }}
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ebs-gp3
              resources:
                requests:
                  storage: 150Gi
        # enable scraping via pod annotations, several deployments use them by default
        # copied from https://github.com/prometheus-community/helm-charts/blob/prometheus-15.0.1/charts/prometheus/values.yaml#L1663-L1708
        additionalScrapeConfigs:
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
            action: replace
            regex: (https?)
            target_label: __scheme__
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
          - source_labels: [__meta_kubernetes_pod_phase]
            regex: Pending|Succeeded|Failed|Completed
            action: drop
        resources:
          requests:
            cpu: 1
            memory: 4800Mi
          limits:
            cpu: 3
            memory: 13000Mi
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
          traefik.ingress.kubernetes.io/router.middlewares: ingress-platform-standard@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt
        ingressClassName: traefik-private
        pathType: Prefix
        hosts:
        - prom.{{ .Environment.Name }}.example.net
        tls:
        - hosts:
          - prom.{{ .Environment.Name }}.example.net
          secretName: prom-tls
    prometheusOperator:
      logFormat: json
      resources:
        requests:
          cpu: 1m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 512Mi
      admissionWebhooks:
        patch:
          resources:
            requests:
              cpu: 1m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 512Mi
    kube-state-metrics:
      autosharding:
        enabled: true
      replicas: 2
      podDisruptionBudget:
        maxUnavailable: 1
      selfMonitor:
        enabled: true
      resources:
        requests:
          cpu: 2m
          memory: 50Mi
        limits:
          cpu: 200m
          memory: 200Mi
    prometheus-node-exporter:
      prometheus:
        monitor:
          # by default there is only 'instance' label with 'ip:port' value
          # additionally provide 'node' label with node's name
          # to be used in Lens and prometheus-adapter
          relabelings:
          - regex: (.+)
            sourceLabels:
            - __meta_kubernetes_endpoint_node_name
            targetLabel: node
      resources:
        requests:
          cpu: 10m
          memory: 16Mi
        limits:
          cpu: 400m
          memory: 100Mi
    kubelet:
      serviceMonitor:
        metricRelabelings:
          # upstream recommended drops, see https://github.com/prometheus-operator/kube-prometheus/blob/v0.11.0/manifests/kubernetesControlPlane-serviceMonitorKubelet.yaml#L14-L46
        - action: drop
          regex: kubelet_(pod_worker_latency_microseconds|pod_start_latency_microseconds|cgroup_manager_latency_microseconds|pod_worker_start_latency_microseconds|pleg_relist_latency_microseconds|pleg_relist_interval_microseconds|runtime_operations|runtime_operations_latency_microseconds|runtime_operations_errors|eviction_stats_age_microseconds|device_plugin_registration_count|device_plugin_alloc_latency_microseconds|network_plugin_operations_latency_microseconds)
          sourceLabels:
          - __name__
        - action: drop
          regex: scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_predicate_evaluation|scheduling_algorithm_priority_evaluation|scheduling_algorithm_preemption_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)
          sourceLabels:
          - __name__
        - action: drop
          regex: apiserver_(request_count|request_latencies|request_latencies_summary|dropped_requests|storage_data_key_generation_latencies_microseconds|storage_transformation_failures_total|storage_transformation_latencies_microseconds|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)
          sourceLabels:
          - __name__
        - action: drop
          regex: kubelet_docker_(operations|operations_latency_microseconds|operations_errors|operations_timeout)
          sourceLabels:
          - __name__
        - action: drop
          regex: reflector_(items_per_list|items_per_watch|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)
          sourceLabels:
          - __name__
        - action: drop
          regex: etcd_(helper_cache_hit_count|helper_cache_miss_count|helper_cache_entry_count|object_counts|request_cache_get_latencies_summary|request_cache_add_latencies_summary|request_latencies_summary)
          sourceLabels:
          - __name__
        - action: drop
          regex: transformation_(transformation_latencies_microseconds|failures_total)
          sourceLabels:
          - __name__
        - action: drop
          regex: (admission_quota_controller_adds|admission_quota_controller_depth|admission_quota_controller_longest_running_processor_microseconds|admission_quota_controller_queue_latency|admission_quota_controller_unfinished_work_seconds|admission_quota_controller_work_duration|APIServiceOpenAPIAggregationControllerQueue1_adds|APIServiceOpenAPIAggregationControllerQueue1_depth|APIServiceOpenAPIAggregationControllerQueue1_longest_running_processor_microseconds|APIServiceOpenAPIAggregationControllerQueue1_queue_latency|APIServiceOpenAPIAggregationControllerQueue1_retries|APIServiceOpenAPIAggregationControllerQueue1_unfinished_work_seconds|APIServiceOpenAPIAggregationControllerQueue1_work_duration|APIServiceRegistrationController_adds|APIServiceRegistrationController_depth|APIServiceRegistrationController_longest_running_processor_microseconds|APIServiceRegistrationController_queue_latency|APIServiceRegistrationController_retries|APIServiceRegistrationController_unfinished_work_seconds|APIServiceRegistrationController_work_duration|autoregister_adds|autoregister_depth|autoregister_longest_running_processor_microseconds|autoregister_queue_latency|autoregister_retries|autoregister_unfinished_work_seconds|autoregister_work_duration|AvailableConditionController_adds|AvailableConditionController_depth|AvailableConditionController_longest_running_processor_microseconds|AvailableConditionController_queue_latency|AvailableConditionController_retries|AvailableConditionController_unfinished_work_seconds|AvailableConditionController_work_duration|crd_autoregistration_controller_adds|crd_autoregistration_controller_depth|crd_autoregistration_controller_longest_running_processor_microseconds|crd_autoregistration_controller_queue_latency|crd_autoregistration_controller_retries|crd_autoregistration_controller_unfinished_work_seconds|crd_autoregistration_controller_work_duration|crdEstablishing_adds|crdEstablishing_depth|crdEstablishing_longest_running_processor_microseconds|crdEstablishing_queue_latency|crdEstablishing_retries|crdEstablishing_unfinished_work_seconds|crdEstablishing_work_duration|crd_finalizer_adds|crd_finalizer_depth|crd_finalizer_longest_running_processor_microseconds|crd_finalizer_queue_latency|crd_finalizer_retries|crd_finalizer_unfinished_work_seconds|crd_finalizer_work_duration|crd_naming_condition_controller_adds|crd_naming_condition_controller_depth|crd_naming_condition_controller_longest_running_processor_microseconds|crd_naming_condition_controller_queue_latency|crd_naming_condition_controller_retries|crd_naming_condition_controller_unfinished_work_seconds|crd_naming_condition_controller_work_duration|crd_openapi_controller_adds|crd_openapi_controller_depth|crd_openapi_controller_longest_running_processor_microseconds|crd_openapi_controller_queue_latency|crd_openapi_controller_retries|crd_openapi_controller_unfinished_work_seconds|crd_openapi_controller_work_duration|DiscoveryController_adds|DiscoveryController_depth|DiscoveryController_longest_running_processor_microseconds|DiscoveryController_queue_latency|DiscoveryController_retries|DiscoveryController_unfinished_work_seconds|DiscoveryController_work_duration|kubeproxy_sync_proxy_rules_latency_microseconds|non_structural_schema_condition_controller_adds|non_structural_schema_condition_controller_depth|non_structural_schema_condition_controller_longest_running_processor_microseconds|non_structural_schema_condition_controller_queue_latency|non_structural_schema_condition_controller_retries|non_structural_schema_condition_controller_unfinished_work_seconds|non_structural_schema_condition_controller_work_duration|rest_client_request_latency_seconds|storage_operation_errors_total|storage_operation_status_count)
          sourceLabels:
          - __name__
        cAdvisorMetricRelabelings:
        # upstream recommended drops, https://github.com/prometheus-operator/kube-prometheus/blob/v0.11.0/manifests/kubernetesControlPlane-serviceMonitorKubelet.yaml#L59-L74
        - action: drop
          regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
          sourceLabels:
          - __name__
        - action: drop
          regex: (container_spec_.*|container_file_descriptors|container_sockets|container_threads_max|container_threads|container_start_time_seconds|container_last_seen);;
          sourceLabels:
          - __name__
          - pod
          - namespace
        - action: drop
          regex: (container_blkio_device_usage_total);.+
          sourceLabels:
          - __name__
          - container
        # drop high cardinality useless label to reduce memory usage
        - action: labeldrop
          regex: ^id$
    kubeApiServer:
      serviceMonitor:
        metricRelabelings:
        # upstream recommended drops, see https://github.com/prometheus-operator/kube-prometheus/blob/v0.11.0/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml#L13-L62
        - action: drop
          regex: kubelet_(pod_worker_latency_microseconds|pod_start_latency_microseconds|cgroup_manager_latency_microseconds|pod_worker_start_latency_microseconds|pleg_relist_latency_microseconds|pleg_relist_interval_microseconds|runtime_operations|runtime_operations_latency_microseconds|runtime_operations_errors|eviction_stats_age_microseconds|device_plugin_registration_count|device_plugin_alloc_latency_microseconds|network_plugin_operations_latency_microseconds)
          sourceLabels:
          - __name__
        - action: drop
          regex: scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_predicate_evaluation|scheduling_algorithm_priority_evaluation|scheduling_algorithm_preemption_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)
          sourceLabels:
          - __name__
        - action: drop
          regex: apiserver_(request_count|request_latencies|request_latencies_summary|dropped_requests|storage_data_key_generation_latencies_microseconds|storage_transformation_failures_total|storage_transformation_latencies_microseconds|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)
          sourceLabels:
          - __name__
        - action: drop
          regex: kubelet_docker_(operations|operations_latency_microseconds|operations_errors|operations_timeout)
          sourceLabels:
          - __name__
        - action: drop
          regex: reflector_(items_per_list|items_per_watch|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)
          sourceLabels:
          - __name__
        - action: drop
          regex: etcd_(helper_cache_hit_count|helper_cache_miss_count|helper_cache_entry_count|object_counts|request_cache_get_latencies_summary|request_cache_add_latencies_summary|request_latencies_summary)
          sourceLabels:
          - __name__
        - action: drop
          regex: transformation_(transformation_latencies_microseconds|failures_total)
          sourceLabels:
          - __name__
        - action: drop
          regex: (admission_quota_controller_adds|admission_quota_controller_depth|admission_quota_controller_longest_running_processor_microseconds|admission_quota_controller_queue_latency|admission_quota_controller_unfinished_work_seconds|admission_quota_controller_work_duration|APIServiceOpenAPIAggregationControllerQueue1_adds|APIServiceOpenAPIAggregationControllerQueue1_depth|APIServiceOpenAPIAggregationControllerQueue1_longest_running_processor_microseconds|APIServiceOpenAPIAggregationControllerQueue1_queue_latency|APIServiceOpenAPIAggregationControllerQueue1_retries|APIServiceOpenAPIAggregationControllerQueue1_unfinished_work_seconds|APIServiceOpenAPIAggregationControllerQueue1_work_duration|APIServiceRegistrationController_adds|APIServiceRegistrationController_depth|APIServiceRegistrationController_longest_running_processor_microseconds|APIServiceRegistrationController_queue_latency|APIServiceRegistrationController_retries|APIServiceRegistrationController_unfinished_work_seconds|APIServiceRegistrationController_work_duration|autoregister_adds|autoregister_depth|autoregister_longest_running_processor_microseconds|autoregister_queue_latency|autoregister_retries|autoregister_unfinished_work_seconds|autoregister_work_duration|AvailableConditionController_adds|AvailableConditionController_depth|AvailableConditionController_longest_running_processor_microseconds|AvailableConditionController_queue_latency|AvailableConditionController_retries|AvailableConditionController_unfinished_work_seconds|AvailableConditionController_work_duration|crd_autoregistration_controller_adds|crd_autoregistration_controller_depth|crd_autoregistration_controller_longest_running_processor_microseconds|crd_autoregistration_controller_queue_latency|crd_autoregistration_controller_retries|crd_autoregistration_controller_unfinished_work_seconds|crd_autoregistration_controller_work_duration|crdEstablishing_adds|crdEstablishing_depth|crdEstablishing_longest_running_processor_microseconds|crdEstablishing_queue_latency|crdEstablishing_retries|crdEstablishing_unfinished_work_seconds|crdEstablishing_work_duration|crd_finalizer_adds|crd_finalizer_depth|crd_finalizer_longest_running_processor_microseconds|crd_finalizer_queue_latency|crd_finalizer_retries|crd_finalizer_unfinished_work_seconds|crd_finalizer_work_duration|crd_naming_condition_controller_adds|crd_naming_condition_controller_depth|crd_naming_condition_controller_longest_running_processor_microseconds|crd_naming_condition_controller_queue_latency|crd_naming_condition_controller_retries|crd_naming_condition_controller_unfinished_work_seconds|crd_naming_condition_controller_work_duration|crd_openapi_controller_adds|crd_openapi_controller_depth|crd_openapi_controller_longest_running_processor_microseconds|crd_openapi_controller_queue_latency|crd_openapi_controller_retries|crd_openapi_controller_unfinished_work_seconds|crd_openapi_controller_work_duration|DiscoveryController_adds|DiscoveryController_depth|DiscoveryController_longest_running_processor_microseconds|DiscoveryController_queue_latency|DiscoveryController_retries|DiscoveryController_unfinished_work_seconds|DiscoveryController_work_duration|kubeproxy_sync_proxy_rules_latency_microseconds|non_structural_schema_condition_controller_adds|non_structural_schema_condition_controller_depth|non_structural_schema_condition_controller_longest_running_processor_microseconds|non_structural_schema_condition_controller_queue_latency|non_structural_schema_condition_controller_retries|non_structural_schema_condition_controller_unfinished_work_seconds|non_structural_schema_condition_controller_work_duration|rest_client_request_latency_seconds|storage_operation_errors_total|storage_operation_status_count)
          sourceLabels:
          - __name__
        - action: drop
          regex: etcd_(debugging|disk|server).*
          sourceLabels:
          - __name__
        - action: drop
          regex: apiserver_admission_controller_admission_latencies_seconds_.*
          sourceLabels:
          - __name__
        - action: drop
          regex: apiserver_admission_step_admission_latencies_seconds_.*
          sourceLabels:
          - __name__
        - action: drop
          regex: apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)
          sourceLabels:
          - __name__
          - le
    # control plane services aren't accessible on EKS
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeEtcd:
      enabled: false
- name: prometheus-pushgateway
  namespace: monitoring
  chart: prometheus-community/prometheus-pushgateway
  version: 1.18.2
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - serviceMonitor:
      enabled: true
    resources:
      requests:
        cpu: 10m
        memory: 30Mi
      limits:
        cpu: 200m
        memory: 50Mi
- name: prometheus-adapter
  namespace: monitoring
  chart: prometheus-community/prometheus-adapter
  version: 3.4.0
  needs:
  - monitoring/prometheus
  values:
  - prometheus:
      url: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local
    # provide resource metrics required for HPA and 'kubectl top'
    # see https://github.com/prometheus-community/helm-charts/tree/prometheus-adapter-3.0.0/charts/prometheus-adapter#resource-metrics
    # configuration based on https://github.com/prometheus-operator/kube-prometheus/blob/v0.9.0/manifests/prometheus-adapter-configMap.yaml
    rules:
      default: false
      resource:
        cpu:
          containerQuery: sum(irate(container_cpu_usage_seconds_total{<<.LabelMatchers>>,container!="",pod!=""}[120s])) by (<<.GroupBy>>)
          nodeQuery: sum(1 - irate(node_cpu_seconds_total{mode="idle"}[60s]) * on(namespace, pod) group_left(node) (node_namespace_pod:kube_pod_info:{<<.LabelMatchers>>})) by (<<.GroupBy>>)
          resources:
            overrides:
              node:
                resource: node
              namespace:
                resource: namespace
              pod:
                resource: pod
          containerLabel: container
        memory:
          containerQuery: sum(container_memory_working_set_bytes{<<.LabelMatchers>>,container!="",pod!=""}) by (<<.GroupBy>>)
          nodeQuery: sum(node_memory_MemTotal_bytes{job="node-exporter",<<.LabelMatchers>>} - node_memory_MemAvailable_bytes{job="node-exporter",<<.LabelMatchers>>}) by (<<.GroupBy>>)
          resources:
            overrides:
              node:
                resource: node
              namespace:
                resource: namespace
              pod:
                resource: pod
          containerLabel: container
        window: 5m
    replicas: 2
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    resources:
      requests:
        cpu: 10m
        memory: 48Mi
      limits:
        cpu: 100m
        memory: 128Mi
- name: promtail
  namespace: logging
  chart: grafana/promtail
  version: 6.2.3
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - config:
      clients:
      - url: https://loki.example.net/loki/api/v1/push
    extraArgs:
    - --client.external-labels=cluster={{ .Environment.Name }}
    serviceMonitor:
      enabled: true
    resources:
      requests:
        cpu: 10m
        memory: 100Mi
      limits:
        cpu: 200m
        memory: 200Mi
- name: cert-manager
  namespace: cert-manager
  chart: jetstack/cert-manager
  version: v1.9.1
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - installCRDs: true
    prometheus:
      servicemonitor:
        enabled: true
    replicaCount: 2
    resources:
      requests:
        cpu: 1m
        memory: 100Mi
      limits:
        cpu: 200m
        memory: 400Mi
    webhook:
      replicaCount: 2
      resources:
        requests:
          cpu: 4m
          memory: 30Mi
        limits:
          cpu: 200m
          memory: 400Mi
    cainjector:
      replicaCount: 2
      resources:
        requests:
          cpu: 3m
          memory: 100Mi
        limits:
          cpu: 200m
          memory: 400Mi
  hooks:
  - events: ["cleanup"] # add sleep after deployment to ensure that
    command: "sleep"    # ValidatingWebhookConfiguration is available for cr
    args: ["30"]        # see https://github.com/jetstack/cert-manager/issues/2908
# TODO PDB policies ideally should be part of cert-manager chart, but for now
# create them manaully, see https://github.com/jetstack/cert-manager/issues/3898
- name: cert-manager-pdb
  namespace: cert-manager
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  needs:
  - cert-manager/cert-manager
  values:
  - resources:
    - apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: cert-manager
      spec:
        maxUnavailable: 1
        selector:
          matchLabels:
            app.kubernetes.io/name: cert-manager
            app.kubernetes.io/instance: cert-manager
    - apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: cert-manager-cainjector
      spec:
        maxUnavailable: 1
        selector:
          matchLabels:
            app.kubernetes.io/name: cainjector
            app.kubernetes.io/instance: cert-manager
    - apiVersion: policy/v1
      kind: PodDisruptionBudget
      metadata:
        name: cert-manager-webhook
      spec:
        maxUnavailable: 1
        selector:
          matchLabels:
            app.kubernetes.io/name: webhook
            app.kubernetes.io/instance: cert-manager
- name: cloudflare-api-token-secret
  namespace: cert-manager
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  values:
  - resources:
    - apiVersion: v1
      kind: Secret
      metadata:
        name: cloudflare-api-token-secret
      type: Opaque
      stringData:
        api-token: dummy
- name: letsencrypt-issuers
  namespace: cert-manager
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  disableValidationOnInstall: true
  needs:
  - cert-manager/cert-manager
  - cert-manager/cloudflare-api-token-secret
  values:
  - resources:
    - apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: letsencrypt-staging
      spec:
        acme:
          server: https://acme-staging-v02.api.letsencrypt.org/directory
          email: devops@example.com
          privateKeySecretRef:
            name: letsencrypt-staging-account-key
          solvers:
          - dns01:
              cloudflare:
                email: dummy
                apiTokenSecretRef:
                  name: cloudflare-api-token-secret
                  key: api-token
    - apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: letsencrypt
      spec:
        acme:
          server: https://acme-v02.api.letsencrypt.org/directory
          email: devops@example.com
          privateKeySecretRef:
            name: letsencrypt-account-key
          solvers:
          - selector:
              dnsZones:
              - 'example.net'
            dns01:
              cloudflare:
                email: dummy
                apiTokenSecretRef:
                  name: cloudflare-api-token-secret
                  key: api-token
- name: selfsigning-issuer
  namespace: cert-manager
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  disableValidationOnInstall: true
  needs:
  - cert-manager/cert-manager
  values:
  - resources:
    - apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: selfsigning
      spec:
        selfSigned: {}
- name: external-dns
  namespace: external-dns
  chart: bitnami/external-dns
  version: 6.7.5
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - provider: cloudflare
    cloudflare:
      apiToken: dummy
      email: dummy
      proxied: false
    policy: sync
    logFormat: json
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
    domainFilters: {{ .Values.externalDns.domainFilters | toYaml | nindent 6 }}
    zoneIdFilters: {{ .Values.externalDns.zoneIdFilters | toYaml | nindent 6 }}
    txtOwnerId: {{ .Environment.Name }}
    replicas: 2
    podDisruptionBudget:
      maxUnavailable: 1
    resources:
      requests:
        cpu: 1m
        memory: 30Mi
      limits:
        cpu: 200m
        memory: 200Mi
- name: traefik-namespace
  namespace: platform
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  values:
  - resources:
    - apiVersion: v1
      kind: Namespace
      metadata:
        name: ingress
        labels:
          # see https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/deploy/pod_readiness_gate/
          elbv2.k8s.aws/pod-readiness-gate-inject: enabled
- name: traefik-public
  namespace: ingress
  chart: traefik/traefik
  version: 10.24.0
  needs:
  - platform/traefik-namespace
  values:
  - deployment:
      replicas: 2
    providers:
      kubernetesIngress:
        # this option is needed to allow using `ExternalName` services as ingress backend
        # which simplifies the migration from swarm to kubernetes. Must be removed after migration is completed
        # due to recommendations described here: https://github.com/kubernetes/kubernetes/issues/103675
        allowExternalNameServices: true
        publishedService:
          enabled: true
    ingressRoute:
      dashboard:
        enabled: false
    ingressClass:
      enabled: true
    globalArguments:
    - --providers.kubernetesingress.ingressclass=traefik-public
    - --api.insecure=true
    - --entrypoints.web.proxyprotocol.insecure=true
    - --entryPoints.web.transport.respondingTimeouts.idleTimeout=300s
    - --entrypoints.websecure.proxyprotocol.insecure=true
    - --entryPoints.websecure.transport.respondingTimeouts.idleTimeout=300s
    - --serversTransport.maxIdleConnsPerHost=100
    - --serversTransport.forwardingTimeouts.idleConnTimeout=900s
    additionalArguments:
    {{- if .Values | get "traefik.common.tracing.enabled" false }}
    - --tracing.jaeger=true
    - --tracing.jaeger.samplingServerURL=http://jaeger-agent.tracing.svc:5778/sampling
    - --tracing.jaeger.localAgentHostPort=jaeger-agent.tracing.svc:6831
    {{- end -}}
    {{- if .Values | get "traefik.public.additionalArguments" false }}
    {{- .Values.traefik.public.additionalArguments | toYaml | nindent 4 }}
    {{- end }}
    pilot:
      dashboard: false
    service:
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "external"
        service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
        service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
        service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
        service.beta.kubernetes.io/aws-load-balancer-name: "k8s-{{ .Environment.Name }}-internet"
        {{- if .Values | get "traefik.common.nlb.dualstack" false }}
        service.beta.kubernetes.io/aws-load-balancer-ip-address-type: "dualstack"
        {{- end }}
    ports:
      websecure:
        tls:
          enabled: true
      {{- if .Values | get "traefik.public.additionalPorts" false }}
      {{- .Values.traefik.public.additionalPorts | toYaml | nindent 6 }}
      {{- end }}
    # applied to both traefik-public and traefik-private
    # see note about 'default' at https://doc.traefik.io/traefik/https/tls/#tls-options
    tlsOptions:
      default:
        minVersion: VersionTLS12
    logs:
      general:
        format: json
        level: INFO
      access:
        format: json
        enabled: true
    metrics:
      prometheus:
        addRoutersLabels: true
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    resources:
      requests:
        cpu: 50m
        memory: 60Mi
      limits:
        cpu: 500m
        memory: 512Mi
- name: traefik-private
  namespace: ingress
  chart: traefik/traefik
  version: 10.24.0
  needs:
  - platform/traefik-namespace
  values:
  - deployment:
      replicas: 2
    providers:
      kubernetesIngress:
        # this option is needed to allow using `ExternalName` services as ingress backend
        # which simplifies the migration from swarm to kubernetes. Must be removed after migration is completed
        # due to recommendations described here: https://github.com/kubernetes/kubernetes/issues/103675
        allowExternalNameServices: true
        publishedService:
          enabled: true
    ingressRoute:
      dashboard:
        enabled: false
    ingressClass:
      enabled: true
    globalArguments:
    - --providers.kubernetesingress.ingressclass=traefik-private
    - --api.insecure=true
    - --entrypoints.web.proxyprotocol.insecure=true
    - --entryPoints.web.transport.respondingTimeouts.idleTimeout=300s
    - --entrypoints.websecure.proxyprotocol.insecure=true
    - --entryPoints.websecure.transport.respondingTimeouts.idleTimeout=300s
    - --serversTransport.maxIdleConnsPerHost=100
    - --serversTransport.forwardingTimeouts.idleConnTimeout=900s
    additionalArguments:
    {{- if .Values | get "traefik.common.tracing.enabled" false }}
    - --tracing.jaeger=true
    - --tracing.jaeger.samplingServerURL=http://jaeger-agent.tracing.svc:5778/sampling
    - --tracing.jaeger.localAgentHostPort=jaeger-agent.tracing.svc:6831
    {{- end }}
    pilot:
      dashboard: false
    service:
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "external"
        service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip"
        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
        service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
        service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
        service.beta.kubernetes.io/aws-load-balancer-name: "k8s-{{ .Environment.Name }}-internal"
    ports:
      websecure:
        tls:
          enabled: true
      {{- if .Values | get "traefik.private.additionalPorts" false }}
      {{- .Values.traefik.private.additionalPorts | toYaml | nindent 6 }}
      {{- end }}
    logs:
      general:
        format: json
        level: INFO
      access:
        format: json
        enabled: true
    metrics:
      prometheus:
        addRoutersLabels: true
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    resources:
      requests:
        cpu: 50m
        memory: 60Mi
      limits:
        cpu: 500m
        memory: 512Mi
- name: traefik-dashboard-ingress
  namespace: ingress
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  needs:
  - ingress/traefik-private
  values:
  - resources:
    - apiVersion: v1
      kind: Service
      metadata:
        name: traefik-public-dashboard
        namespace: ingress
      spec:
        selector:
          app.kubernetes.io/name: traefik
          app.kubernetes.io/instance: traefik-public
        ports:
        - name: traefik
          port: 9000
          targetPort: traefik
      name:
    - apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: traefik-public-dashboard
        namespace: ingress
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
          traefik.ingress.kubernetes.io/router.middlewares: ingress-platform-standard@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt
      spec:
        ingressClassName: traefik-private # make it accessible only within VPC
        rules:
        - host: traefik-public.{{ .Environment.Name }}.k8s.example.net
          http:
            paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: traefik-public-dashboard
                  port:
                    name: traefik
        tls:
        - hosts:
          - traefik-public.{{ .Environment.Name }}.k8s.example.net
          secretName: traefik-public-dashboard-tls
    - apiVersion: v1
      kind: Service
      metadata:
        name: traefik-private-dashboard
        namespace: ingress
      spec:
        selector:
          app.kubernetes.io/name: traefik
          app.kubernetes.io/instance: traefik-private
        ports:
        - name: traefik
          port: 9000
          targetPort: traefik
    - apiVersion: networking.k8s.io/v1
      kind: Ingress
      metadata:
        name: traefik-private-dashboard
        namespace: ingress
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
          traefik.ingress.kubernetes.io/router.middlewares: ingress-platform-standard@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt
      spec:
        ingressClassName: traefik-private # make it accessible only within VPC
        rules:
        - host: traefik-private.{{ .Environment.Name }}.k8s.example.net
          http:
            paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: traefik-private-dashboard
                  port:
                    name: traefik
        tls:
        - hosts:
          - traefik-private.{{ .Environment.Name }}.k8s.example.net
          secretName: traefik-private-dashboard-tls
- name: traefik-middlewares
  namespace: ingress
  chart: deliveryhero/k8s-resources
  version: 0.5.5
  disableValidationOnInstall: true
  needs:
  - ingress/traefik-public
  values:
  - CustomResources:
    - apiVersion: traefik.containo.us/v1alpha1
      kind: Middleware
      fullnameOverride: redirect-to-https
      spec:
        redirectScheme:
          scheme: https
          permanent: true
    - apiVersion: traefik.containo.us/v1alpha1
      kind: Middleware
      fullnameOverride: hsts-header
      spec:
        headers:
          stsSeconds: 31536000
          stsIncludeSubdomains: true
    - apiVersion: traefik.containo.us/v1alpha1
      kind: Middleware
      fullnameOverride: compress
      spec:
        compress: {}
    - apiVersion: traefik.containo.us/v1alpha1
      kind: Middleware
      fullnameOverride: platform-standard
      spec:
        chain:
          middlewares:
          - name: redirect-to-https
          - name: hsts-header
          - name: compress
- name: snapshot-validation-webhook
  namespace: kube-system # TODO move to 'platform' namespace
  chart: piraeus-charts/snapshot-validation-webhook
  version: 1.5.2
  disableValidationOnInstall: true
  needs:
  - cert-manager/cert-manager
  - cert-manager/selfsigning-issuer
  values:
  - replicaCount: 2
    tls:
      certManagerIssuerRef:
        name: selfsigning
        kind: ClusterIssuer
    resources:
      requests:
        cpu: 1m
        memory: 8Mi
      limits:
        cpu: 200m
        memory: 200Mi
- name: snapshot-controller
  namespace: kube-system # TODO move to 'platform' namespace
  chart: piraeus-charts/snapshot-controller
  version: 1.5.1
  disableValidationOnInstall: true
  needs:
  - kube-system/snapshot-validation-webhook
  values:
  - replicaCount: 2
    resources:
      requests:
        cpu: 1m
        memory: 10Mi
      limits:
        cpu: 200m
        memory: 200Mi
- name: aws-ebs-csi-driver
  namespace: kube-system # TODO move to 'platform' namespace
  chart: aws-ebs-csi-driver/aws-ebs-csi-driver
  version: 2.10.0
  needs:
  - kube-system/snapshot-controller
  values:
  - controller:
      serviceAccount:
        annotations:
          eks.amazonaws.com/role-arn: dummy
      resources:
        requests:
          cpu: 1m
          memory: 70Mi
        limits:
          cpu: 200m
          memory: 400Mi
    node:
      resources:
        requests:
          cpu: 1m
          memory: 20Mi
        limits:
          cpu: 200m
          memory: 400Mi
- name: ebs-sc
  namespace: kube-system
  chart: incubator/raw # TODO: migrate away from deprecated chart
  version: 0.2.4
  disableValidationOnInstall: true
  needs:
  - kube-system/aws-ebs-csi-driver
  values:
  - resources:
    - apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: ebs-gp3
      provisioner: ebs.csi.aws.com
      allowVolumeExpansion: true
      reclaimPolicy: Retain
      volumeBindingMode: WaitForFirstConsumer
      parameters:
        type: gp3
        encrypted: "true"
        allowAutoIOPSPerGBIncrease: "true"
        tagSpecification_1: EnvName={{ .Environment.Name }}
        tagSpecification_2: EnvType={{ .Values.envType }}
        tagSpecification_3: Owner=platform@example.com
        tagSpecification_4: RepoUrl=gitlab.example.net/devops/k8s-platform
- name: aws-efs-csi-driver
  namespace: platform
  chart: aws-efs-csi-driver/aws-efs-csi-driver
  version: 2.2.7
  values:
  - controller:
      tags:
        EnvName: {{ .Environment.Name }}
        EnvType: {{ .Values.envType }}
        Owner: platform@example.com
        RepoUrl: gitlab.example.net/devops/k8s-platform
      deleteAccessPointRootDir: true
      volMetricsOptIn: true
      resources:
        requests:
          cpu: 1m
          memory: 20Mi
        limits:
          cpu: 200m
          memory: 400Mi
      serviceAccount:
        annotations:
          eks.amazonaws.com/role-arn: dummy
    node:
      resources:
        requests:
          cpu: 1m
          memory: 20Mi
        limits:
          cpu: 200m
          memory: 400Mi
      serviceAccount:
        annotations:
          eks.amazonaws.com/role-arn: dummy
    sidecars:
      livenessProbe:
        resources:
          requests:
            cpu: 1m
            memory: 20Mi
          limits:
            cpu: 200m
            memory: 400Mi
      nodeDriverRegistrar:
        resources:
          requests:
            cpu: 1m
            memory: 20Mi
          limits:
            cpu: 200m
            memory: 400Mi
      csiProvisioner:
        resources:
          requests:
            cpu: 1m
            memory: 20Mi
          limits:
            cpu: 200m
            memory: 400Mi
    storageClasses:
    - name: efs
      parameters:
        provisioningMode: efs-ap
        fileSystemId: dummy
        directoryPerms: "700"
      reclaimPolicy: Delete
      volumeBindingMode: Immediate
- name: aws-load-balancer-controller
  namespace: kube-system # TODO move to 'platform' namespace
  chart: eks/aws-load-balancer-controller
  version: 1.4.4
  disableValidationOnInstall: true
  needs:
  - cert-manager/cert-manager
  - monitoring/prometheus
  values:
  - enableCertManager: true
    clusterName: "{{ .Environment.Name }}-cluster" # need to keep `-cluster` suffix here to comply with aws eks cluster name
    defaultTags:
      EnvName: {{ .Environment.Name }}
      EnvType: {{ .Values.envType }}
      Owner: devops@example.com
      RepoUrl: example.net/devops
    createIngressClassResource: false
    ingressClassParams:
      create: false
    serviceAccount:
      name: aws-load-balancer-controller
      annotations:
        eks.amazonaws.com/role-arn: dummy
    serviceMonitor:
      enabled: true
    podDisruptionBudget:
      maxUnavailable: 1
    resources:
      requests:
        cpu: 1m
        memory: 100Mi
      limits:
        cpu: 200m
        memory: 200Mi
- name: cluster-autoscaler
  namespace: platform
  chart: autoscaler/cluster-autoscaler
  version: 9.19.3
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - image:
      tag: v1.23.1 # autoscaler image tag should match current minor kubernetes version
    autoDiscovery:
      clusterName: "{{ .Environment.Name }}-cluster" # need to keep `-cluster` suffix here to comply with aws eks cluster name
    cloudProvider: aws
    awsRegion: "" # let IRSA webhook to set it
    extraArgs:
      balance-similar-node-groups: true
      skip-nodes-with-system-pods: false
      skip-nodes-with-local-storage: false
      cordon-node-before-terminating: true
      daemonset-eviction-for-occupied-nodes: false
      # ignore node labels that prevent assuming node groups are similiar
      # needed because we're using MNG with mixed node types and EBS CSI controller
      balancing-ignore-label_1: node.kubernetes.io/instance-type
      balancing-ignore-label_2: beta.kubernetes.io/instance-type
      balancing-ignore-label_3: topology.ebs.csi.aws.com/zone
      balancing-ignore-label_4: eks.amazonaws.com/nodegroup
    fullnameOverride: cluster-autoscaler # generated fullname 'cluster-autoscaler-aws-cluster-autoscaler' is senseless here, make it simplier
    replicaCount: 2
    rbac:
      serviceAccount:
        annotations:
          eks.amazonaws.com/role-arn: dummy
    resources:
      requests:
        cpu: 5m
        memory: 150Mi
      limits:
        cpu: 200m
        memory: 500Mi
    serviceMonitor:
      enabled: true
# overprovision cluster by 3 CPU cores and 12Gi of memory by running 3 expandable 'pause' pods with 1 CPU and 4Gi of memory each
# this is static rule, so no matter what cluster is in size, there are always this amount of resources overprovisioned
# see https://github.com/kubernetes/autoscaler/blob/cluster-autoscaler-1.21.1/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler
- name: overprovisioning
  namespace: platform
  chart: deliveryhero/cluster-overprovisioner
  version: 0.7.9
  values:
  - deployments:
    - name: overprovision
      replicaCount: 3
      resources:
        requests:
          cpu: 1
          memory: 4Gi
        limits:
          cpu: 1
          memory: 4Gi
    serviceAccount:
      create: false
- name: rbac-manager
  namespace: rbac-manager
  chart: fairwinds-stable/rbac-manager
  version: 1.13.1
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - serviceMonitor:
      enabled: true
    resources:
      requests:
        cpu: 1m
        memory: 16Mi
      limits:
        cpu: 100m
        memory: 200Mi
- name: kyverno
  namespace: kyverno
  chart: kyverno/kyverno
  version: v2.5.2
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - replicaCount: 3
    podDisruptionBudget:
      enabled: true
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
      limits:
        cpu: 1
        memory: 500Mi
    serviceMonitor:
      enabled: true
  hooks:
  - events: ["cleanup"] # add sleep after deployment to ensure that admission webhook
    command: "sleep"    # is able to retrieve the complete list of server APIs
    args: ["60"]        # see https://github.com/kyverno/kyverno/issues/1490
- name: policy-reporter
  namespace: kyverno
  chart: kyverno-policy-reporter/policy-reporter
  version: 2.11.0
  disableValidationOnInstall: true
  needs:
  - kyverno/kyverno
  - monitoring/prometheus
  - ingress/traefik-private
  values:
  - kyvernoPlugin:
      enabled: true
      resources:
        requests:
          cpu: 1m
          memory: 10Mi
        limits:
          cpu: 100m
          memory: 100Mi
    monitoring:
      enabled: true
      grafana:
        dashboards:
          enabled: false
    ui:
      enabled: true
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.entrypoints: web, websecure
          traefik.ingress.kubernetes.io/router.middlewares: ingress-platform-standard@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt
        className: traefik-private
        hosts:
        - host: policy.{{ .Environment.Name }}.k8s.example.net
          paths:
          - path: /
            pathType: Prefix
        tls:
        - hosts:
          - policy.{{ .Environment.Name }}.k8s.example.net
          secretName: policy-reporter-tls
      resources:
        requests:
          cpu: 1m
          memory: 10Mi
        limits:
          cpu: 50m
          memory: 100Mi
    global:
      plugins:
        kyverno: true
    target:
      ui:
        minimumPriority: ""
        skipExistingOnStartup: false
    resources:
      requests:
        cpu: 1m
        memory: 100Mi
      limits:
        cpu: 100m
        memory: 512Mi
- name: kyverno-policies
  namespace: kyverno
  chart: kyverno/kyverno-policies
  version: v2.5.2
  disableValidationOnInstall: true
  needs:
  - kyverno/kyverno
  values:
  - policyExclude:
      disallow-host-path:
        any:
        - resources:
            namespaces:
            - logging
            - monitoring
            - platform
      disallow-host-ports:
        any:
        - resources:
            namespaces:
            - monitoring
      disallow-host-namespaces:
        any:
        - resources:
            namespaces:
            - monitoring
      disallow-privileged-containers:
        any:
        - resources:
            namespaces:
            - platform
- name: kyverno-custom-policies
  namespace: kyverno
  chart: deliveryhero/k8s-resources
  version: 0.5.5
  disableValidationOnInstall: true
  needs:
  - kyverno/kyverno
  values:
  - Secrets:
    - fullnameOverride: example-image-pull
      type: kubernetes.io/dockerconfigjson
      b64Keys:
        .dockerconfigjson: |-
          {}
    CustomResources:
    - apiVersion: kyverno.io/v1
      kind: ClusterPolicy
      fullnameOverride: example-image-pull
      annotations:
        policies.kyverno.io/title: Sync Image Pull Secret
        policies.kyverno.io/category: Sync
        policies.kyverno.io/subject: Image Pull Secret
        policies.kyverno.io/description: >-
          This policy will copy a secret called "example-image-pull"
          which exists in the kyverno namespace to new namespaces when
          they are created. It will also push updates to the copied secrets
          should the source secret be changed.
      spec:
        generateExistingOnPolicyUpdate: true
        rules:
        - name: sync-example-image-pull-secret
          match:
            any:
            - resources:
                kinds:
                - Namespace
                selector:
                  matchExpressions:
                  - { key: example.com/env-type, operator: In, values: [ poc, dev, stage, prod ] }
          generate:
            apiVersion: v1
            kind: Secret
            name: example-image-pull
            namespace: {{`"{{request.object.metadata.name}}"`}}
            synchronize: true
            clone:
              namespace: kyverno
              name: example-image-pull
    - apiVersion : kyverno.io/v1
      kind: ClusterPolicy
      fullnameOverride: require-requests-limits
      annotations:
        policies.kyverno.io/title: Require Resource Limits and Requests
        policies.kyverno.io/category: Best Practices
        policies.kyverno.io/severity: medium
        policies.kyverno.io/subject: Pod
        policies.kyverno.io/description: >-
          As application workloads share cluster resources, it is important to limit resources
          requested and consumed by each pod. This policy validates that all container specs
          set CPU and memory resources for requests and limits.
      spec:
        validationFailureAction: enforce
        background: false
        rules:
        - name: validate-regular-resources
          match:
            any:
            - resources:
                kinds:
                - DaemonSet
                - Deployment
                - StatefulSet
          # sonobuoy e2e tests designed in a way where not all tasks have
          # resources defined, we just have to exclude them from policy
          exclude:
            any:
            - resources:
                namespaces:
                - "sonobuoy-*"
            - resources:
                namespaceSelector:
                  matchLabels:
                    e2e-framework: "*"
            - resources:
                namespaceSelector:
                  matchLabels:
                    e2e-run: "*"
          validate:
            message: CPU and memory resource requests and limits are required.
            pattern:
              spec:
                template:
                  spec:
                    containers:
                    - name: ?*
                      resources:
                        limits:
                          cpu: ?*
                          memory: ?*
                        requests:
                          cpu: ?*
                          memory: ?*
        - name: validate-cronjob-resource
          match:
            any:
            - resources:
                kinds:
                - CronJob
          # sonobuoy e2e tests designed in a way where not all tasks have
          # resources defined, we just have to exclude them from policy
          exclude:
            any:
            - resources:
                namespaces:
                - "sonobuoy-*"
            - resources:
                namespaceSelector:
                  matchLabels:
                    e2e-framework: "*"
            - resources:
                namespaceSelector:
                  matchLabels:
                    e2e-run: "*"
          validate:
            message: CPU and memory resource requests and limits are required.
            pattern:
              spec:
                jobTemplate:
                  spec:
                    template:
                      spec:
                        containers:
                        - name: "?*"
                          resources:
                            limits:
                              cpu: "?*"
                              memory: "?*"
                            requests:
                              cpu: "?*"
                              memory: "?*"
- name: node-problem-detector
  namespace: platform
  chart: deliveryhero/node-problem-detector
  version: 2.2.2
  disableValidationOnInstall: true
  needs:
  - monitoring/prometheus
  values:
  - settings:
      log_monitors:
      - /config/abrt-adaptor.json
      - /config/kernel-monitor.json
      - /config/systemd-monitor.json
      custom_plugin_monitors:
      - /config/kernel-monitor-counter.json
      - /config/systemd-monitor-counter.json
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
      prometheusRule:
        enabled: true
    resources:
      requests:
        cpu: 1m
        memory: 6Mi
      limits:
        cpu: 100m
        memory: 100Mi
    extraVolumes:
    - name: kmsg
      hostPath:
        path: /dev/kmsg
    extraVolumeMounts:
    - name: kmsg
      mountPath: /dev/kmsg
      readOnly: true
- name: harbor-webhook
  chart: indeedeng-alpha/harbor-container-webhook
  version: 0.3.4
  namespace: platform
  disableValidationOnInstall: true
  needs:
  - cert-manager/cert-manager
  values:
  - replicaCount: 2
    resources:
      requests:
        cpu: 10m
        memory: 20Mi
      limits:
        cpu: 200m
        memory: 500Mi
    extraArgs:
    - --zap-log-level=info
    rules:
    - name: 'docker.io rewrite rule'
      # it's able to handle bare images like 'ubuntu:latest' too
      matches:
      - '^docker.io'
      replace: 'cr.example.net/dockerhub-proxy'
      # if harbor is unavailable/unreachable, don't rewrite anything
      checkUpstream: true
